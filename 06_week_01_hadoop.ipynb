{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Hadoop Ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start SSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's get ssh server service up, since hadoop components communicate over ssh protocol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo service ssh stop\n",
    "sudo service ssh start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check that sshd works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service ssh status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check some environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $HADOOP_HOME\n",
    "echo $HADOOP_CONF_DIR\n",
    "echo $HADOOP_PREFIX\n",
    "echo $PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view important configuration files. This is a very simple configuration for standalone mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls $HADOOP_CONF_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat $HADOOP_CONF_DIR/core-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat $HADOOP_CONF_DIR/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat $HADOOP_CONF_DIR/mapred-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat $HADOOP_CONF_DIR/slaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before first use, the hdfs must be formatted. We do not do it now, since data is already imported into hdfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "#yes Y | hdfs namenode -format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start and stop HDFS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the hdfs services:\n",
    "\n",
    "The\n",
    "```Bash\n",
    "2>&1 | grep -Pv \"^WARNING\"\n",
    "```\n",
    "part is there for suppressing annoying WARNING messages in the standard error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start-dfs.sh 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which hadoop services run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of hdfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfsadmin -report 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the services you will use these commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logs exist at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /opt/hadoop-2.9.2/logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create a test file and import into hdfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"this a test file\" > deneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat deneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -put ~/deneme / 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check the file system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls / 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the contents of the file in the hdfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -cat /deneme 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a copy of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -cp /deneme /deneme2 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that it is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls / 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory named somedir in hdfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -mkdir /somedir 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that it is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls / 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move deneme2 into somedir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -mv /deneme2 /somedir 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the contents of somedir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /somedir 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now export somedir from hdfs to local file system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -get /somedir ~/ 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether it exists in the local file system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ~/somedir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the disk usage of files and directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -du /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What exists under /data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the imdb, comtrade_s1 and he_sisli datasets from previous sessions\n",
    "\n",
    "- ncdc is a part of a huge dataset on detailed meteorological data for USA beginning from 1901\n",
    "- ngrams is a dataset of the words that appeared in books at books.google.com\n",
    "\n",
    "We will try to make use of all this data in this and the following session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now delete the directory /somedir in hdfs. Note that we should pass the recursive (-r) option just as we should do in the local file system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -rm -r /somedir 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs fsck / 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the report at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl \"http://localhost:50070/fsck?ugi=hadoop&path=%2F\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access a remote or local system though the web UI, you can point your browser to the above link (not in binder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More commands are listed at:\n",
    "\n",
    "https://hadoop.apache.org/docs/r2.9.2/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html\n",
    "\n",
    "https://hadoop.apache.org/docs/r2.9.2/hadoop-project-dist/hadoop-common/FileSystemShell.html\n",
    "\n",
    "https://data-flair.training/blogs/top-hadoop-hdfs-commands-tutorial/\n",
    "\n",
    "https://www.edureka.co/blog/hdfs-commands-hadoop-shell-command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Bash\n",
    "Usage: hadoop fs [generic options]\n",
    "\t[-appendToFile <localsrc> ... <dst>]\n",
    "\t[-cat [-ignoreCrc] <src> ...]\n",
    "\t[-checksum <src> ...]\n",
    "\t[-chgrp [-R] GROUP PATH...]\n",
    "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
    "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
    "\t[-copyFromLocal [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n",
    "\t[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
    "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] <path> ...]\n",
    "\t[-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]\n",
    "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
    "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
    "\t[-df [-h] [<path> ...]]\n",
    "\t[-du [-s] [-h] [-x] <path> ...]\n",
    "\t[-expunge]\n",
    "\t[-find <path> ... <expression> ...]\n",
    "\t[-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
    "\t[-getfacl [-R] <path>]\n",
    "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
    "\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\n",
    "\t[-help [cmd ...]]\n",
    "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path> ...]]\n",
    "\t[-mkdir [-p] <path> ...]\n",
    "\t[-moveFromLocal <localsrc> ... <dst>]\n",
    "\t[-moveToLocal <src> <localdst>]\n",
    "\t[-mv <src> ... <dst>]\n",
    "\t[-put [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n",
    "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
    "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n",
    "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
    "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
    "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
    "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
    "\t[-stat [format] <path> ...]\n",
    "\t[-tail [-f] <file>]\n",
    "\t[-test -[defsz] <path>]\n",
    "\t[-text [-ignoreCrc] <src> ...]\n",
    "\t[-touchz <path> ...]\n",
    "\t[-truncate [-w] <length> <path> ...]\n",
    "\t[-usage [cmd ...]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 1:**\n",
    "\n",
    "Use above commands at least once for your own example\n",
    "- Create a test file\n",
    "- Import into hdfs\n",
    "- Create a directory\n",
    "- Move and copy files around\n",
    "- Cat the files\n",
    "- Export from hdfs into local file system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start yarn services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether resource manager works by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop the service by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Reduce Job: Word Count on NCDC Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through two examples using the 1932-1936 years of the ncdc weather data set:\n",
    "\n",
    "- Word Count\n",
    "- Max Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is the \"hello world\" of map reduce and also cited inside the official documentation\n",
    "\n",
    "We will first implement the map reduce job as a unix command with pipes\n",
    "\n",
    "If this works right, we will run it using hadoop streaming\n",
    "\n",
    "Two versions will be run:\n",
    "- One with a mapper and reducer,\n",
    "- And the other with a mapper, combiner and reducer\n",
    "\n",
    "In larger datasets run on many nodes, it is good practice to use a combiner so that the network traffic between the mapper and reducer is minimized\n",
    "\n",
    "Note that all codes/commands must accept data from stdin and emit the result to stdout\n",
    "It is best to delimit fields by tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapper phase creates a key value pair out of the data. In the wc example, the original files are send to stdout \n",
    "\n",
    "\"head\" command exists just in order to limit the visible output and will not be a part of the mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ~/data/ncdc/* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the reducer will count the lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ~/data/ncdc/* | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert this into a simple map reduce job:\n",
    "\n",
    "- Note that the output folder should not exist, so flush it before running the job\n",
    "- If the command takes parameters, wrap it around single or double quotes\n",
    "- If the command includes pipes, do it like:\n",
    "- `bash -c \"your command | second command\"`\n",
    "\n",
    "- Note that input and output paths are inside the hdfs\n",
    "- Mapper and reducer paths are inside the main filesystem\n",
    "- Run these commands inside the docker shell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create a directory for outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -mkdir -p /output 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory for outputs from jobs with ncdc dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -mkdir -p /output/ncdc 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting a new job, always make sure the output directory is empty otherwise an error is returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -rm -r -f /output/ncdc/* 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path to hadoop-streaming jar file is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.9.2.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the paths to cat and wc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "which cat\n",
    "which wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output directory should be non-existent before the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.9.2.jar \\\n",
    "-input /data/ncdc \\\n",
    "-output /output/ncdc/1 \\\n",
    "-mapper /usr/bin/cat \\\n",
    "-reducer '/usr/bin/wc -l' \\\n",
    "2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -cat /output/ncdc/1/* 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is in line with what he got from the local run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will rewrite the job adding a combiner:\n",
    "\n",
    "The combiner will take over the job of reducer: For each task, the word count is calculated\n",
    "Now the reducer will just add the word counts!\n",
    "\n",
    "From the shell, the \"bc\" command will do it for us.\n",
    "\n",
    "First let's see what happens until the reducer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in /data/ncdc/txt/*; \\ # each file is send to cat separately\n",
    "                                  # and we see what mapper, combiner and reducer does\n",
    "# do cat $file | \\ # mapper\n",
    "# wc -l; done # combiner\n",
    "\n",
    "for file in ~/data/ncdc/*; do cat $file | wc -l; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, add the reducer:\n",
    "- \"paste -sd+\" puts a \"+\" sign between the numbers\n",
    "- bc will calculate this formula, and add the numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in ~/data/ncdc/*; do cat $file | wc -l; done | paste -sd+ | bc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert it to a mapreduce job:\n",
    "\n",
    "Note that we have a new output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.9.2.jar \\\n",
    "-input /data/ncdc \\\n",
    "-output /output/ncdc/2 \\\n",
    "-mapper /usr/bin/cat \\\n",
    "-combiner '/usr/bin/wc -l' \\\n",
    "-reducer \"bash -c 'paste -sd+ | bc'\" \\\n",
    "2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -cat /output/ncdc/2/* 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title Count Across Years Using IMDB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will revisit the all friend imdb\n",
    "\n",
    "We will first split the title.basics file into 5 equal parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tldr split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the title.basics.tsv into 5 parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p ~/split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/split && split -n 5 ~/data/imdb/tsv/title.basics.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ~/split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -put ~/split /data 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /data 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You job is to \n",
    "- Get the start year column of the files as the mapper\n",
    "- Get the count of each year as the combiner\n",
    "- Aggregate the count of years in each task as the reducer\n",
    "\n",
    "You can use the \"cut\" command to get the necessary column. Note that default field delimiter for cut is \"\\t\", the same as the files\n",
    "\n",
    "You can view the initial rows of the file with head, so that you decide on which column to extract\n",
    "\n",
    "For combiner and reducer, you can use a small but very talented tool called \"q\" inside sandbox. \"q\" uses sqlite as its backend, however, it does not need a database: It can work on columnar data fed from stdin. Usual sql statements just work!\n",
    "\n",
    "Only you should write \"-\" for the \"from\" clause and fields are named as c1, c2, etc. You can use select, from, group by and aggeragete functions such as count() or sum().\n",
    "\n",
    "Note that when feeding into combiner or reducer you should wrap the line inside quotes. But \n",
    "the statement itself needs quotes. So one of the quote pair should be single and the other should be double as such: -combiner 'q \"select ........\"'\n",
    "\n",
    "The output path you provide must be non-existent. So either provide a new one or flush the existing one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First try the mapper, combiner and reducer on the command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`for file in ~/split/*; do your_mapper_command | head -3; echo; done`\n",
    "\n",
    "1894\n",
    "1892\n",
    "1892\n",
    "\n",
    "2000\n",
    "2000\n",
    "2000\n",
    "\n",
    "2013\n",
    "2011\n",
    "2011\n",
    "\n",
    "2014\n",
    "1909\n",
    "2014\n",
    "\n",
    "2016\n",
    "2016\n",
    "2016\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the combiner (\"head\" and \"column\" are there just for visual purposes):\n",
    "\n",
    "`for file in ~/split/*; do your_mapper_command $file | \\\n",
    "q \"your_combiner_sql_statement\" | head -20 | column -c 150; done`\n",
    "\n",
    "`Warning: column count is one - did you provide the correct delimiter?\n",
    "1888 2\t\t1891 7\t\t1894 66\t\t1897 798\t1900 811\t1903 1642\t1906 508\n",
    "1889 1\t\t1892 9\t\t1895 65\t\t1898 1049\t1901 924\t1904 492\t1907 553\n",
    "1890 3\t\t1893 2\t\t1896 466\t1899 893\t1902 879\t1905 307\n",
    "Warning: column count is one - did you provide the correct delimiter?\n",
    "1888 1\t\t1895 3\t\t1898 86\t\t1901 155\t1904 551\t1907 823\t1910 2633\n",
    "1890 1\t\t1896 105\t1899 174\t1902 224\t1905 482\t1908 1689\t1911 3076\n",
    "1894 4\t\t1897 117\t1900 180\t1903 224\t1906 524\t1909 2016\n",
    "Warning: column count is one - did you provide the correct delimiter?\n",
    "1874 1\t\t1888 2\t\t1891 2\t\t1896 250\t1899 719\t1902 702\t1905 892\n",
    "1878 1\t\t1889 1\t\t1894 25\t\t1897 407\t1900 823\t1903 795\t1906 822\n",
    "1887 1\t\t1890 1\t\t1895 47\t\t1898 603\t1901 673\t1904 783\n",
    "Warning: column count is one - did you provide the correct delimiter?\n",
    "1883 1\t\t1896 4\t\t1899 5\t\t1904 1\t\t1908 56\t\t1911 426\t1914 666\n",
    "1890 1\t\t1897 4\t\t1900 9\t\t1905 23\t\t1909 1581\t1912 612\t1915 517\n",
    "1891 1\t\t1898 2\t\t1903 1\t\t1907 7\t\t1910 1465\t1913 473\n",
    "Warning: column count is one - did you provide the correct delimiter?\n",
    "1887 1\t1897 16\t1899 9\t1901 7\t1903 14\t1905 2\t1907 4\t1909 8\t1911 15\t1913 78\n",
    "1896 7\t1898 30\t1900 11\t1902 3\t1904 3\t1906 3\t1908 3\t1910 10\t1912 65\t1914 49\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last, we add our reducer:\n",
    "\n",
    "`for file in ~/split/*; do your_mapper_command $file | \\\n",
    "q \"your_combiner_sql_statement\"; done | \\\n",
    "q \"your_reducer_sql_statement\" | column -c 100`\n",
    "\n",
    "`Warning: column count is one - did you provide the correct delimiter?\n",
    "Warning: column count is one - did you provide the correct delimiter?\n",
    "Warning: column count is one - did you provide the correct delimiter?\n",
    "Warning: column count is one - did you provide the correct delimiter?\n",
    "Warning: column count is one - did you provide the correct delimiter?\n",
    "1874 1\t\t1909 5420\t1934 2701\t1959 13273\t1984 23243\t2009 160879\n",
    "1878 1\t\t1910 6410\t1935 2645\t1960 14479\t1985 24729\t2010 178864\n",
    "1883 1\t\t1911 6439\t1936 3081\t1961 14573\t1986 25012\t2011 207730\n",
    "1887 2\t\t1912 8459\t1937 3341\t1962 13627\t1987 26746\t2012 229315\n",
    "1888 5\t\t1913 9562\t1938 3191\t1963 15111\t1988 25883\t2013 242918\n",
    "1889 2\t\t1914 9003\t1939 2828\t1964 16602\t1989 27896\t2014 248265\n",
    "1890 6\t\t1915 8428\t1940 2396\t1965 18361\t1990 29530\t2015 251666\n",
    "1891 10\t\t1916 6959\t1941 2379\t1966 19025\t1991 30996\t2016 253051\n",
    "1892 9\t\t1917 5524\t1942 2277\t1967 19709\t1992 32802\t2017 162619\n",
    "1893 2\t\t1918 4608\t1943 2064\t1968 18150\t1993 35493\t2018 11721\n",
    "1894 95\t\t1919 3974\t1944 1895\t1969 19139\t1994 39719\t2019 975\n",
    "1895 115\t1920 4425\t1945 1855\t1970 19248\t1995 46688\t2020 212\n",
    "1896 832\t1921 4177\t1946 2248\t1971 19507\t1996 47151\t2021 34\n",
    "1897 1342\t1922 3566\t1947 2716\t1972 18904\t1997 51891\t2022 25\n",
    "1898 1770\t1923 3004\t1948 3296\t1973 19964\t1998 58519\t2023 5\n",
    "1899 1800\t1924 3057\t1949 4310\t1974 19458\t1999 62518\t2024 3\n",
    "1900 1834\t1925 3288\t1950 5374\t1975 20122\t2000 66473\t2025 1\n",
    "1901 1759\t1926 3016\t1951 6333\t1976 19101\t2001 74113\t2026 1\n",
    "1902 1808\t1927 3100\t1952 7119\t1977 19296\t2002 77652\t2115 1\n",
    "1903 2676\t1928 3065\t1953 7824\t1978 19401\t2003 86811\tN 321491\n",
    "1904 1830\t1929 3149\t1954 8276\t1979 20030\t2004 102515\n",
    "1905 1706\t1930 2778\t1955 9502\t1980 20921\t2005 115374\n",
    "1906 1857\t1931 2779\t1956 10716\t1981 19853\t2006 129299\n",
    "1907 2481\t1932 2783\t1957 12085\t1982 20390\t2007 143551\n",
    "1908 4274\t1933 2635\t1958 12564\t1983 21264\t2008 151624\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run it as a mapreduce job on hdfs and yarn, before that we flush the output  directory (or pass a non-existent one)\n",
    "\n",
    "```Bash\n",
    "hdfs dfs -mkdir -p /output/split\n",
    "hdfs dfs -rm -r -f /output/split/*\n",
    "```\n",
    "\n",
    "And run you mapreduce job:\n",
    "\n",
    "```Bash\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.9.2.jar \\\n",
    "-input /data/split \\\n",
    "-output /output/split/1 \\\n",
    "-mapper \"your_mapper_command\" \\\n",
    "-combiner 'q \"your_combiner_sql_statement\"' \\\n",
    "-reducer 'q \"your_reducer_sql_statement\"'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Scripts for MapReduce Job: Maximum Temperature Example on NCDC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the same NCDC data set we'll recreate the maximum temperature example from the Elephant Book as a \"Hadoop streaming\" job:\n",
    "\n",
    "Your computers have a century of the temperature data of USA. We will use just the first 10 years of this data\n",
    "\n",
    "And for each year we will get the max temperature\n",
    "\n",
    "We will first start with standard unix tools\n",
    "\n",
    "I played a bit with the original script given as a part of the supplementary material for the Elephant Book\n",
    "\n",
    "Note that there may be empty files and the job must have a remedy for this issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script is as follows:\n",
    "\n",
    "```Bash\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# adjusted by Serhat Cevikel to measure the time\n",
    "\n",
    "#START=$(date +%s.%N)\n",
    "path=$1\n",
    "starty=$2\n",
    "endy=$3\n",
    "\n",
    "years=$(seq $starty $endy)\n",
    "\n",
    "\n",
    "for year in $years\n",
    "do\n",
    "    filee=\"${path}/${year}\"\n",
    "  echo -ne `basename $year `\"\\t\"\n",
    "  cat $filee | \\ \n",
    "    awk '{ temp = substr($0, 88, 5) + 0;\n",
    "           q = substr($0, 93, 1);\n",
    "           if (temp !=9999 && q ~ /[01459]/ && temp > max) max = temp }\n",
    "         END { print max }'\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing temp values are coded as 9999 and they are excluded\n",
    "\"q\" is a quality code and should be one of 0,1,4,5,9 to be included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path to script is ~/codes/hadoop_max_temperature.sh\n",
    "\n",
    "In my implementation it takes 3 parameters: the path to gz files, start year and end year\n",
    "\n",
    "The data resides at ~/data/ncdc\n",
    "\n",
    "Let's do it for 1932 to 1936"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "~/codes/hadoop_max_temperature.sh \\\n",
    "~/data/ncdc \\\n",
    "1901 1910"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's transform it to a map reduce job:\n",
    "\n",
    "In the map phase, the key value pairs are extracted from the data: the year and temp reading\n",
    "\n",
    "In the reduce phase the max temp for each year is calculated\n",
    "\n",
    "mapper is:\n",
    "\n",
    "```R\n",
    "#!/usr/bin/Rscript\n",
    "\n",
    "con <- file(\"stdin\")\n",
    "#con <- file(\"1910\")\n",
    "liness <- readLines(con)\n",
    "close(con)\n",
    "\n",
    "year <- as.numeric(substr(liness, 16, 19))\n",
    "temp <- as.numeric(substr(liness, 88, 92))\n",
    "qq <- as.numeric(substr(liness, 93, 93))\n",
    "\n",
    "output <- cbind(year, temp)\n",
    "\n",
    "output <- output[temp != 9999 & qq %in% c(0, 1, 4, 5, 9),]\n",
    "\n",
    "for (i in seq_along(output[,1]))\n",
    "{\n",
    "    pasted <- paste(output[i,], collapse = \"\\t\")\n",
    "    cat(sprintf(\"%s\\n\", pasted))\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code accepts input from stdin so can be used similar to the previous one - before Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ~/data/ncdc/{1901..1910} | ~/codes/mapper.R | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reducer code is as follows:\n",
    "\n",
    "```R\n",
    "#!/usr/bin/Rscript\n",
    "\n",
    "con <- file(\"stdin\")\n",
    "#com <- file(\"mapped\")\n",
    "liness <- readLines(con)\n",
    "close(con)\n",
    "\n",
    "keyval <- list()\n",
    "\n",
    "for (i in seq_along(liness))\n",
    "{\n",
    "    linex <- unlist(strsplit(liness[i], split = \"\\t\"))\n",
    "    key <- linex[1]\n",
    "    val <- as.numeric(linex[2])\n",
    "\n",
    "    cur.maxval <- keyval[[key]]\n",
    "\n",
    "    if (is.null(cur.maxval))\n",
    "    {   \n",
    "        keyval[[key]] <- val \n",
    "    }\n",
    "    else\n",
    "    {   \n",
    "        if (val > cur.maxval)\n",
    "        {\n",
    "            keyval[[key]] <- val \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "keys <- as.numeric(names(keyval))\n",
    "vals <- as.numeric(unlist(keyval))\n",
    "\n",
    "output <- matrix(c(keys, vals), ncol = 2)\n",
    "output <- output[order(keys),, drop = F]\n",
    "\n",
    "for (i in seq_along(output[,1]))\n",
    "{\n",
    "    pasted <- paste(output[i,], collapse = \"\\t\")\n",
    "    cat(sprintf(\"%s\\n\", pasted))\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ~/data/ncdc/{1901..1910} | ~/codes/mapper.R | ~/codes/reducer.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the map reduce job. Note that we have to pass the custom script files via \"-file\" option so that all nodes can run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.9.2.jar \\\n",
    "-input /data/ncdc/{1901..1910} \\\n",
    "-output /output/ncdc/3 \\\n",
    "-mapper ~/codes/mapper.R \\\n",
    "-reducer ~/codes/reducer.R \\\n",
    "-file ~/codes/mapper.R \\\n",
    "-file ~/codes/reducer.R \\\n",
    "2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can run it with a combiner (the same script as the reducer for this case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.9.2.jar \\\n",
    "-input /data/ncdc/{1901..1910} \\\n",
    "-output /output/ncdc/3 \\\n",
    "-mapper ~/codes/mapper.R \\\n",
    "-combiner ~/codes/reducer.R \\\n",
    "-reducer ~/codes/reducer.R \\\n",
    "-file ~/codes/mapper.R \\\n",
    "-file ~/codes/reducer.R \\\n",
    "2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use sqoop in order to import data\n",
    "- from RDBMS\n",
    "- into HDFS as text files\n",
    "- or as hive tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's start postgresql service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo service postgresql start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether it runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -c \"\\l\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's remember the table names in imdb2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d imdb2 -c \"\\dt+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see whether sqoop can connect to the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqoop list-tables --connect jdbc:postgresql://localhost:5432/imdb2 \\\n",
    "--username postgres 2>&1 | grep -Pv \"^(Warning|Please)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, first import a single table as a text file into hdfs\n",
    "\n",
    "The target directory should be non-existent. The direct flag is for fast imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqoop import --connect jdbc:postgresql://localhost:5432/imdb2 \\\n",
    "--username postgres \\\n",
    "--table name_basics \\\n",
    "--target-dir /import \\\n",
    "--direct \\\n",
    "2>&1 | grep -Pv \"^(Warning|Please)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check whether the new directory and file(s) exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /import 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And view the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -cat /import/part-m-00000 2>&1 | grep -Pv \"^WARNING\" | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get the file out of hdfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -get /import/part-m-00000 . 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head part-m-00000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
