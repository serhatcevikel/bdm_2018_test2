{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HADOOP ECOSYSTEM CONTINUED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start the daemons again:\n",
    "\n",
    "First ssh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for s: \r\n"
     ]
    }
   ],
   "source": [
    "sudo service ssh stop\n",
    "sudo service ssh start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service ssh status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then hdfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start-dfs.sh 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And yarn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether hdfs and yarn daemons are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And postgresql:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo service postgresql start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -c \"\\l\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive is a data warehouse system and MapReduce wrapper that presents an SQL interface. So SQL programmers feel at home through Hive in the Hadoop Ecosystem, without writing MapReduce jobs explicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT DATA WITH SQOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sqoop can import the data as Hive tables with \"--hive-import --create-hive-table\" flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqoop import \\\n",
    "--connect jdbc:postgresql://localhost:5432/imdb2 \\\n",
    "--username postgres \\\n",
    "--table name_basics \\\n",
    "--hive-import --create-hive-table --direct \\\n",
    "2>&1 | grep -Pv \"^(Warning|Please|WARNING)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And view the imported file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /apps/hive/warehouse/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's check from hive whether files are imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e 'show tables'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to read the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -cat /apps/hive/warehouse/name_basics/* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to delete the table, we do it from the hive command, not by manually deleting the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e 'drop table name_basics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e 'show tables'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /apps/hive/warehouse/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And no more name_basics directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import title_basics into hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqoop import \\\n",
    "--connect jdbc:postgresql://localhost:5432/imdb2 \\\n",
    "--username postgres \\\n",
    "--table title_basics \\\n",
    "--hive-import --create-hive-table --direct \\\n",
    "2>&1 | grep -Pv \"^(Warning|Please|WARNING)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether they are imported into hive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e 'show tables'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /apps/hive/warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A SQOOP TO HIVE IMPORT EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll \n",
    "- first import a publicly available sql dump into postgresql (inside sandbox-hdp)\n",
    "- Then import the postgresql database into hdfs as text files and hive tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database is \"World\", containing list of cities, countries and languages. The link is\n",
    "\n",
    "http://pgfoundry.org/frs/download.php/527/world-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget http://pgfoundry.org/frs/download.php/527/world-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And extract the archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar -xzvf world-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new database at postgresql:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createdb -U postgres world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And import the dump into the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres world < world.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d world -c \"\\dt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And view the fields of the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d world -c \"\\d+ public.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And view some of the rows of the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d world -c \"select * from city limit 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d world -c \"select * from country limit 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d world -c \"select * from countrylanguage limit 10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's import into hive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqoop import \\\n",
    "--connect jdbc:postgresql://localhost:5432/world \\\n",
    "--username postgres \\\n",
    "--hive-import --create-hive-table --direct \\\n",
    "2>&1 | grep -Pv \"^(Warning|Please|WARNING)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First show tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"show tables\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's create a database from hive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"create database deneme\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"show databases\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete deneme database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"drop database deneme\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a database to hold imdb tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"create database imdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a copy of the title_basics inside imdb database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"create table imdb.title_basics\n",
    "as select * from title_basics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANd show the tables inside imdb database: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"show tables in imdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's run a simple query inside a hive database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"use imdb; select count(*) from title_basics;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a hive database on hdfs, similar to our postgresql database, we can run similar queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "- get the original titles, start year and runtime minutes of\n",
    "- titles before 1930 and\n",
    "- longer than 100 minutes\n",
    "- genres include Drama\n",
    "- limit to the first 10 results\n",
    "\n",
    "Note that instead of tilde (~), we should use LIKE in order to make a partial match in string fields to account for the flavor difference b/w PostgreSQL and MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"use imdb;\n",
    "\n",
    "SELECT originaltitle, startyear, runtimeminutes\n",
    "  FROM title_basics\n",
    "  WHERE startyear <= 1930\n",
    "  AND runtimeminutes > 100\n",
    "  AND genres LIKE 'Drama'\n",
    "  LIMIT 10;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, under the hood, Hive converts the HiveQL query to a series of map reduce jobs\n",
    "\n",
    "The plan of the conversion can be viewed by prefixing the statement with \"explain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"use imdb;\n",
    "\n",
    "explain\n",
    "SELECT originaltitle, startyear, runtimeminutes\n",
    "  FROM title_basics\n",
    "  WHERE startyear <= 1930\n",
    "  AND runtimeminutes > 100\n",
    "  AND genres LIKE 'Drama'\n",
    "  LIMIT 10;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive is suitable for simpler queries. However as the queries get more complex and need a clearer definition of the dataflow, we should revert to a tool such as Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A HIVE exercise with world database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous sqoop example, you were required to import the tables from the \"world\" database into hive\n",
    "\n",
    "Now first please create a \"world\" database and copy the three tables into that database in hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in the last example we had provided \"use imdb;\" as the namespace\n",
    "\n",
    "In order to refer to tables not attached to a custom database that we created yet, \"use default;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can refer to those databases as default.DBNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"create database world;\n",
    "\n",
    "use default;\n",
    "create table world.city as select * from default.city;\n",
    "create table world.country as select * from default.country;\n",
    "create table world.countrylanguage as select * from default.countrylanguage;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the table sin the world database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"show tables in world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"show create table world.city;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order get information about tables, you can run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will return information on the schema (column names and types), file type and size information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can repeat it for other tables in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run a query to see the average lifeexpantancy of all countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"use world;\n",
    "select avg(lifeexpectancy) from country;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the next task is:\n",
    "\n",
    "- get the names (from country table) of the countries, official languages (from country languages) of which include english "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "OK\n",
    "American Samoa\n",
    "Anguilla\n",
    "Antigua and Barbuda\n",
    "Australia\n",
    "Barbados\n",
    "Belize\n",
    "Bermuda\n",
    "United Kingdom\n",
    "Virgin Islands, British\n",
    "Cayman Islands\n",
    "South Africa\n",
    "Falkland Islands\n",
    "Gibraltar\n",
    "Guam\n",
    "Hong Kong\n",
    "Ireland\n",
    "Christmas Island\n",
    "Canada\n",
    "Cocos (Keeling) Islands\n",
    "Lesotho\n",
    "Malta\n",
    "Marshall Islands\n",
    "Montserrat\n",
    "Nauru\n",
    "Niue\n",
    "Norfolk Island\n",
    "Palau\n",
    "Northern Mariana Islands\n",
    "Saint Helena\n",
    "Saint Kitts and Nevis\n",
    "Saint Lucia\n",
    "Saint Vincent and the Grenadines\n",
    "Samoa\n",
    "Seychelles\n",
    "Tokelau\n",
    "Tonga\n",
    "Turks and Caicos Islands\n",
    "Tuvalu\n",
    "New Zealand\n",
    "Vanuatu\n",
    "United States\n",
    "Virgin Islands, U.S.\n",
    "Zimbabwe\n",
    "United States Minor Outlying Islands\n",
    "Time taken: 11.424 seconds, Fetched: 44 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"use world;\n",
    "select c.name from country c left join\n",
    "countrylanguage l on c.code=l.countrycode\n",
    "where l.isofficial = true\n",
    "and l.language = 'English';\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pig is a scripting language for creating workflows based on MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pig transforms the declarative nature of Hive into a procedural one, so that dataflow steps are more easily defined "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this examples, we will use HCatalog to connect Pig to Hive databases. HCatalog is a table and storage management layer for Hadoop that enables users with different data processing tools — Pig, MapReduce — to more easily read and write data on the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable pig with HCatalog:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pig -useHCatalog\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write down the steps defining the workflow and the plan will be executed when we enter the DUMP command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, let's load city table from world database in Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will list the steps and then run them as a single script in batch mode:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "city = LOAD 'world.city' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's select the cities where the countrycode is TUR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "cityturkey = filter city by countrycode == 'TUR';\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's select the cities with a population larger than 1 million"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "largecitytur = filter cityturkey by population > 1000000;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's execute the plan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "DUMP largecitytur;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pig -useHCatalog <<EOF\n",
    "city = LOAD 'world.city' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "cityturkey = filter city by countrycode == 'TUR';\n",
    "largecitytur = filter cityturkey by population > 1000000;\n",
    "DUMP largecitytur;\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import other tables in the world database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "country = LOAD 'world.country' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "\n",
    "\n",
    "lang = LOAD 'world.countrylanguage' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our previous example in Hive as a Pig dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, filter the lang table for countries, official languages of which include English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "codeen = filter lang by language == 'English' and isofficial == true;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we join the filtered countrylanguage and country tables on the coeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "joinen = JOIN country by code, codeen by countrycode;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I select only the name field to be dumped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "names = foreach joinen generate name;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can execute the plan to dump the names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "DUMP names;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pig -useHCatalog <<EOF\n",
    "country = LOAD 'world.country' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "lang = LOAD 'world.countrylanguage' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "codeen = filter lang by language == 'English' and isofficial == true;\n",
    "joinen = JOIN country by code, codeen by countrycode;\n",
    "names = foreach joinen generate name;\n",
    "DUMP names;\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIG EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as an exercise, we will compare the lifeexpentancy of the whole sample and the life expectancy of the countries with English as official language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel of calculating averages in Pig, below is the solution for the first part:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "-- filter for null values\n",
    "countrynotnull = filter country by lifeexpectancy is not null;\n",
    "\n",
    "-- get lifeexpectancy column\n",
    "\n",
    "lifeall = foreach countrynotnull generate lifeexpectancy;\n",
    "\n",
    "-- combine values into a single group \n",
    "lifeallg = group lifeall all;\n",
    "\n",
    "-- calculate the average\n",
    "avgall = foreach lifeallg generate AVG(lifeall);\n",
    "-- execute\n",
    "DUMP avgall;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is:\n",
    "```Pig\n",
    "(66.486036036036)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pig -useHCatalog <<EOF\n",
    "countrynotnull = filter country by lifeexpectancy is not null;\n",
    "lifeall = foreach countrynotnull generate lifeexpectancy;\n",
    "lifeallg = group lifeall all;\n",
    "avgall = foreach lifeallg generate AVG(lifeall);\n",
    "DUMP avgall;\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now play with code in previous example to get the lifeexpectancy values of English speaking countries (Note that we had extracted the names column. Just change the column)\n",
    "\n",
    "And apply the steps above (you will have the life expectancies of anglophone countries instead of all countries, rest is the same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that null elimination step is not necessary, the result is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should be:\n",
    "```Pig\n",
    "(71.5027027027027)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pig -useHCatalog <<EOF\n",
    "    codeen = filter lang by language == 'English' and isofficial == true and lifeexpectancy is not null;\n",
    "    joinen = JOIN country by code, codeen by countrycode;\n",
    "    lifeen = foreach joinen generate lifeexpectancy;\n",
    "\n",
    "    lifeeng = group lifeen all;\n",
    "    avgen = foreach lifeeng generate AVG(lifeen);\n",
    "\n",
    "    DUMP avgen;\n",
    "EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
