{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HADOOP ECOSYSTEM CONTINUED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start the daemons again:\n",
    "\n",
    "First ssh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo service ssh stop\n",
    "sudo service ssh start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service ssh status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then hdfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start-dfs.sh 2>&1 | grep -Pv \"^(WARNING|SLF4J)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And yarn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether hdfs and yarn daemons are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And postgresql:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo service postgresql start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d imdb2 -c \"\\dt+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive is a data warehouse system and MapReduce wrapper that presents an SQL interface. So SQL programmers feel at home through Hive in the Hadoop Ecosystem, without writing MapReduce jobs explicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT DATA WITH SQOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sqoop can import the data as Hive tables with \"--hive-import --create-hive-table\" flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqoop import \\\n",
    "--connect jdbc:postgresql://localhost:5432/imdb2 \\\n",
    "--username postgres \\\n",
    "--table title_ratings \\\n",
    "--hive-import --create-hive-table --direct \\\n",
    "2>&1 | grep -Pv \"^(Warning|Please|WARNING|SLF4J)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And view the imported file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -find / -name \"title*\" 2>&1 | grep -Pv \"^(Warning|Please|WARNING|SLF4J)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /user/hive/warehouse/ 2>&1 | grep -Pv \"^(Warning|Please|WARNING|SLF4J)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's check from hive whether files are imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hive -e 'show tables' 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to read the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -cat /user/hive/warehouse/title_ratings/* 2>&1 | grep -Pv \"^(Warning|Please|WARNING|SLF4J)\" | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to delete the table, we do it from the hive command, not by manually deleting the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e 'drop table title_ratings' 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e 'show tables' 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /user/hive/warehouse/ 2>&1 | grep -Pv \"^(Warning|Please|WARNING|SLF4J)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A SQOOP TO HIVE IMPORT EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll \n",
    "- first import a publicly available sql dump into postgresql\n",
    "- Then import the postgresql database into hdfs as text files and hive tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database is \"World\", containing list of cities, countries and languages. The link is\n",
    "\n",
    "http://pgfoundry.org/frs/download.php/527/world-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget http://pgfoundry.org/frs/download.php/527/world-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And extract the archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar -xzvf world-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new database at postgresql:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createdb -U postgres world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And import the dump into the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres world < dbsamples-0.1/world/world.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d world -c \"\\dt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And view the fields of the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d world -c \"\\d+ public.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And view some of the rows of the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d world -c \"select * from city limit 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d world -c \"select * from country limit 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d world -c \"select * from countrylanguage limit 10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's import into hive:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First get the list of tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables=$(psql -U postgres -d world -t --pset=\"border=0\" -c \"\\dt\" | \\\n",
    "awk -F \" \" '{ print $2 }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"$tables\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a hive database to import into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"create database world\" 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And import each table in the postgresql database into hive database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"$tables\" | while read l;\n",
    "do\n",
    "    sqoop import \\\n",
    "    --connect jdbc:postgresql://localhost:5432/world \\\n",
    "    --username postgres \\\n",
    "    --table $l \\\n",
    "    --hive-import \\\n",
    "    --create-hive-table \\\n",
    "    --hive-table world.$l \\\n",
    "    --direct \\\n",
    "    2>&1 | grep -Pv \"^(Warning|Please|WARNING|SLF4J)\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HIVE OPERATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a database from hive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"create database deneme\" 2>&1 | grep -Pv \"^(Warning|Please|WARNING|SLF4J)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"show databases\" 2>&1 | grep -Pv \"^(Warning|Please|WARNING|SLF4J)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete deneme database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"drop database deneme\" 2>&1 | grep -Pv \"^(Warning|Please|WARNING|SLF4J)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A HIVE EXERCISE WITH WORLD DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the tables in the world database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"show tables in world\" 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order get information about tables, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"show create table world.city;\" 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will return information on the schema (column names and types), file type and size information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can repeat it for other tables in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a hive database on hdfs, similar to our postgresql database, we can run similar queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run a very simple query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"use world;\n",
    "select * from country limit 1;\" 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, under the hood, Hive converts the HiveQL query to a series of map reduce jobs\n",
    "\n",
    "The plan of the conversion can be viewed by prefixing the statement with \"explain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"use world;\n",
    "explain\n",
    "select * from country limit 1;\" 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive is suitable for simpler queries. However as the queries get more complex and need a clearer definition of the dataflow, we should revert to a tool such as Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run a query to see the average lifeexpantancy of all countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"use world;\n",
    "select avg(lifeexpectancy) from country limit 10;\" 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the next task is:\n",
    "\n",
    "- get the names (from country table) of the countries, official languages (from country languages) of which include english "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "OK\n",
    "American Samoa\n",
    "Anguilla\n",
    "Antigua and Barbuda\n",
    "Australia\n",
    "Barbados\n",
    "Belize\n",
    "Bermuda\n",
    "United Kingdom\n",
    "Virgin Islands, British\n",
    "Cayman Islands\n",
    "South Africa\n",
    "Falkland Islands\n",
    "Gibraltar\n",
    "Guam\n",
    "Hong Kong\n",
    "Ireland\n",
    "Christmas Island\n",
    "Canada\n",
    "Cocos (Keeling) Islands\n",
    "Lesotho\n",
    "Malta\n",
    "Marshall Islands\n",
    "Montserrat\n",
    "Nauru\n",
    "Niue\n",
    "Norfolk Island\n",
    "Palau\n",
    "Northern Mariana Islands\n",
    "Saint Helena\n",
    "Saint Kitts and Nevis\n",
    "Saint Lucia\n",
    "Saint Vincent and the Grenadines\n",
    "Samoa\n",
    "Seychelles\n",
    "Tokelau\n",
    "Tonga\n",
    "Turks and Caicos Islands\n",
    "Tuvalu\n",
    "New Zealand\n",
    "Vanuatu\n",
    "United States\n",
    "Virgin Islands, U.S.\n",
    "Zimbabwe\n",
    "United States Minor Outlying Islands\n",
    "Time taken: 11.424 seconds, Fetched: 44 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"use world;\n",
    "select c.name from country c left join\n",
    "countrylanguage l on c.code=l.countrycode\n",
    "where l.isofficial = true\n",
    "and l.language = 'English';\"  2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pig is a scripting language for creating workflows based on MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pig transforms the declarative nature of Hive into a procedural one, so that dataflow steps are more easily defined "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this examples, we will use HCatalog to connect Pig to Hive databases. HCatalog is a table and storage management layer for Hadoop that enables users with different data processing tools — Pig, MapReduce — to more easily read and write data on the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable pig with HCatalog:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pig -useHCatalog\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write down the steps defining the workflow and the plan will be executed when we enter the DUMP command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, let's load city table from world database in Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will list the steps and then run them as a single script in batch mode:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "city = LOAD 'world.city' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's select the cities where the countrycode is TUR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "cityturkey = filter city by countrycode == 'TUR';\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's select the cities with a population larger than 1 million"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "largecitytur = filter cityturkey by population > 1000000;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's execute the plan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "DUMP largecitytur;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $HADOOP_CLASSPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$PIG_HOME/lib:/opt/pig/lib/hadoop2-runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export PIG_CLASSPATH=$PIG_HOME/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export HCAT_HOME=$HIVE_HOME/hcatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pig -useHCatalog <<EOF\n",
    "city = LOAD 'world.city' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "cityturkey = filter city by countrycode == 'TUR';\n",
    "largecitytur = filter cityturkey by population > 1000000;\n",
    "DUMP largecitytur;\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import other tables in the world database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "country = LOAD 'world.country' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "\n",
    "\n",
    "lang = LOAD 'world.countrylanguage' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our previous example in Hive as a Pig dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, filter the lang table for countries, official languages of which include English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "codeen = filter lang by language == 'English' and isofficial == true;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we join the filtered countrylanguage and country tables on the coeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "joinen = JOIN country by code, codeen by countrycode;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I select only the name field to be dumped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "names = foreach joinen generate name;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can execute the plan to dump the names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "DUMP names;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pig -useHCatalog <<EOF\n",
    "country = LOAD 'world.country' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "lang = LOAD 'world.countrylanguage' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "codeen = filter lang by language == 'English' and isofficial == true;\n",
    "joinen = JOIN country by code, codeen by countrycode;\n",
    "names = foreach joinen generate name;\n",
    "DUMP names;\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIG EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as an exercise, we will compare the lifeexpentancy of the whole sample and the life expectancy of the countries with English as official language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel of calculating averages in Pig, below is the solution for the first part:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "-- filter for null values\n",
    "countrynotnull = filter country by lifeexpectancy is not null;\n",
    "\n",
    "-- get lifeexpectancy column\n",
    "\n",
    "lifeall = foreach countrynotnull generate lifeexpectancy;\n",
    "\n",
    "-- combine values into a single group \n",
    "lifeallg = group lifeall all;\n",
    "\n",
    "-- calculate the average\n",
    "avgall = foreach lifeallg generate AVG(lifeall);\n",
    "-- execute\n",
    "DUMP avgall;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is:\n",
    "```Pig\n",
    "(66.486036036036)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pig -useHCatalog <<EOF\n",
    "countrynotnull = filter country by lifeexpectancy is not null;\n",
    "lifeall = foreach countrynotnull generate lifeexpectancy;\n",
    "lifeallg = group lifeall all;\n",
    "avgall = foreach lifeallg generate AVG(lifeall);\n",
    "DUMP avgall;\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now play with code in previous example to get the lifeexpectancy values of English speaking countries (Note that we had extracted the names column. Just change the column)\n",
    "\n",
    "And apply the steps above (you will have the life expectancies of anglophone countries instead of all countries, rest is the same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that null elimination step is not necessary, the result is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should be:\n",
    "```Pig\n",
    "(71.5027027027027)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pig -useHCatalog <<EOF\n",
    "    codeen = filter lang by language == 'English' and isofficial == true and lifeexpectancy is not null;\n",
    "    joinen = JOIN country by code, codeen by countrycode;\n",
    "    lifeen = foreach joinen generate lifeexpectancy;\n",
    "\n",
    "    lifeeng = group lifeen all;\n",
    "    avgen = foreach lifeeng generate AVG(lifeen);\n",
    "\n",
    "    DUMP avgen;\n",
    "EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
